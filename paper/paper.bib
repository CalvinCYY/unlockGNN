
@article{battaglia_relational_2018,
	title = {Relational inductive biases, deep learning, and graph networks},
	url = {http://arxiv.org/abs/1806.01261},
	abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
	urldate = {2020-09-18},
	journal = {arXiv:1806.01261 [cs, stat]},
	author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
	month = oct,
	year = {2018},
	note = {arXiv: 1806.01261},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/awsm/snap/zotero-snap/common/Zotero/storage/6FC9TDEH/Battaglia et al. - 2018 - Relational inductive biases, deep learning, and gr.pdf:application/pdf;arXiv.org Snapshot:/home/awsm/snap/zotero-snap/common/Zotero/storage/79QIYSEQ/1806.html:text/html}
}

@article{chen_graph_2019,
	title = {Graph {Networks} as a {Universal} {Machine} {Learning} {Framework} for {Molecules} and {Crystals}},
	volume = {31},
	issn = {0897-4756},
	url = {https://doi.org/10.1021/acs.chemmater.9b01294},
	doi = {10.1021/acs.chemmater.9b01294},
	abstract = {Graph networks are a new machine learning (ML) paradigm that supports both relational reasoning and combinatorial generalization. Here, we develop universal MatErials Graph Network (MEGNet) models for accurate property prediction in both molecules and crystals. We demonstrate that the MEGNet models outperform prior ML models such as the SchNet in 11 out of 13 properties of the QM9 molecule data set. Similarly, we show that MEGNet models trained on ∼60 000 crystals in the Materials Project substantially outperform prior ML models in the prediction of the formation energies, band gaps, and elastic moduli of crystals, achieving better than density functional theory accuracy over a much larger data set. We present two new strategies to address data limitations common in materials science and chemistry. First, we demonstrate a physically intuitive approach to unify four separate molecular MEGNet models for the internal energy at 0 K and room temperature, enthalpy, and Gibbs free energy into a single free energy MEGNet model by incorporating the temperature, pressure, and entropy as global state inputs. Second, we show that the learned element embeddings in MEGNet models encode periodic chemical trends and can be transfer-learned from a property model trained on a larger data set (formation energies) to improve property models with smaller amounts of data (band gaps and elastic moduli).},
	number = {9},
	urldate = {2020-09-18},
	journal = {Chem. Mater.},
	author = {Chen, Chi and Ye, Weike and Zuo, Yunxing and Zheng, Chen and Ong, Shyue Ping},
	month = may,
	year = {2019},
	note = {Publisher: American Chemical Society},
	pages = {3564--3572},
	file = {Submitted Version:/home/awsm/snap/zotero-snap/common/Zotero/storage/5D2IA8AF/Chen et al. - 2019 - Graph Networks as a Universal Machine Learning Fra.pdf:application/pdf}
}

@article{tran_methods_2020,
	title = {Methods for comparing uncertainty quantifications for material property predictions},
	url = {http://arxiv.org/abs/1912.10066},
	abstract = {Data science and informatics tools have been proliferating recently within the computational materials science and catalysis fields. This proliferation has spurned the creation of various frameworks for automated materials screening, discovery, and design. Underpinning these frameworks are surrogate models with uncertainty estimates on their predictions. These uncertainty estimates are instrumental for determining which materials to screen next, but the computational catalysis field does not yet have a standard procedure for judging the quality of such uncertainty estimates. Here we present a suite of figures and performance metrics derived from the machine learning community that can be used to judge the quality of such uncertainty estimates. This suite probes the accuracy, calibration, and sharpness of a model quantitatively. We then show a case study where we judge various methods for predicting density-functional-theory-calculated adsorption energies. Of the methods studied here, we find that the best performer is a model where a convolutional neural network is used to supply features to a Gaussian process regressor, which then makes predictions of adsorption energies along with corresponding uncertainty estimates.},
	urldate = {2020-09-20},
	journal = {arXiv:1912.10066 [cond-mat, physics:physics]},
	author = {Tran, Kevin and Neiswanger, Willie and Yoon, Junwoong and Zhang, Qingyang and Xing, Eric and Ulissi, Zachary W.},
	month = feb,
	year = {2020},
	note = {arXiv: 1912.10066},
	keywords = {Condensed Matter - Materials Science, Physics - Computational Physics},
	file = {arXiv Fulltext PDF:/home/awsm/snap/zotero-snap/common/Zotero/storage/JSUA2MND/Tran et al. - 2020 - Methods for comparing uncertainty quantifications .pdf:application/pdf;arXiv.org Snapshot:/home/awsm/snap/zotero-snap/common/Zotero/storage/YYRY27PZ/1912.html:text/html}
}

@article{xie_crystal_2018,
	title = {Crystal {Graph} {Convolutional} {Neural} {Networks} for an {Accurate} and {Interpretable} {Prediction} of {Material} {Properties}},
	volume = {120},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.120.145301},
	doi = {10.1103/PhysRevLett.120.145301},
	abstract = {The use of machine learning methods for accelerating the design of crystalline materials usually requires manually constructed feature vectors or complex transformation of atom coordinates to input the crystal structure, which either constrains the model to certain crystal types or makes it difficult to provide chemical insights. Here, we develop a crystal graph convolutional neural networks framework to directly learn material properties from the connection of atoms in the crystal, providing a universal and interpretable representation of crystalline materials. Our method provides a highly accurate prediction of density functional theory calculated properties for eight different properties of crystals with various structure types and compositions after being trained with 104 data points. Further, our framework is interpretable because one can extract the contributions from local chemical environments to global properties. Using an example of perovskites, we show how this information can be utilized to discover empirical rules for materials design.},
	number = {14},
	urldate = {2020-09-29},
	journal = {Phys. Rev. Lett.},
	author = {Xie, Tian and Grossman, Jeffrey C.},
	month = apr,
	year = {2018},
	note = {Publisher: American Physical Society},
	pages = {145301},
	file = {APS Snapshot:/home/awsm/snap/zotero-snap/common/Zotero/storage/DP2FWRXT/PhysRevLett.120.html:text/html;Full Text:/home/awsm/snap/zotero-snap/common/Zotero/storage/CC88IDVJ/Xie and Grossman - 2018 - Crystal Graph Convolutional Neural Networks for an.pdf:application/pdf}
}

@article{ramprasad_machine_2017,
	title = {Machine learning in materials informatics: recent applications and prospects},
	volume = {3},
	copyright = {2017 The Author(s)},
	issn = {2057-3960},
	shorttitle = {Machine learning in materials informatics},
	url = {https://www.nature.com/articles/s41524-017-0056-5},
	doi = {10.1038/s41524-017-0056-5},
	abstract = {Propelled partly by the Materials Genome Initiative, and partly by the algorithmic developments and the resounding successes of data-driven efforts in other domains, informatics strategies are beginning to take shape within materials science. These approaches lead to surrogate machine learning models that enable rapid predictions based purely on past data rather than by direct experimentation or by computations/simulations in which fundamental equations are explicitly solved. Data-centric informatics methods are becoming useful to determine material properties that are hard to measure or compute using traditional methods—due to the cost, time or effort involved—but for which reliable data either already exists or can be generated for at least a subset of the critical cases. Predictions are typically interpolative, involving fingerprinting a material numerically first, and then following a mapping (established via a learning algorithm) between the fingerprint and the property of interest. Fingerprints, also referred to as “descriptors”, may be of many types and scales, as dictated by the application domain and needs. Predictions may also be extrapolative—extending into new materials spaces—provided prediction uncertainties are properly taken into account. This article attempts to provide an overview of some of the recent successful data-driven “materials informatics” strategies undertaken in the last decade, with particular emphasis on the fingerprint or descriptor choices. The review also identifies some challenges the community is facing and those that should be overcome in the near future.},
	language = {en},
	number = {1},
	urldate = {2020-09-29},
	journal = {npj Computational Materials},
	author = {Ramprasad, Rampi and Batra, Rohit and Pilania, Ghanshyam and Mannodi-Kanakkithodi, Arun and Kim, Chiho},
	month = dec,
	year = {2017},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {1--13},
	file = {Full Text PDF:/home/awsm/snap/zotero-snap/common/Zotero/storage/LS3DPC8F/Ramprasad et al. - 2017 - Machine learning in materials informatics recent .pdf:application/pdf;Snapshot:/home/awsm/snap/zotero-snap/common/Zotero/storage/4KRS4E2M/s41524-017-0056-5.html:text/html}
}

@article{xue_accelerated_2016,
	title = {Accelerated search for materials with targeted properties by adaptive design},
	volume = {7},
	copyright = {2016 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/ncomms11241},
	doi = {10.1038/ncomms11241},
	abstract = {Finding new materials with targeted properties has traditionally been guided by intuition, and trial and error. With increasing chemical complexity, the combinatorial possibilities are too large for an Edisonian approach to be practical. Here we show how an adaptive design strategy, tightly coupled with experiments, can accelerate the discovery process by sequentially identifying the next experiments or calculations, to effectively navigate the complex search space. Our strategy uses inference and global optimization to balance the trade-off between exploitation and exploration of the search space. We demonstrate this by finding very low thermal hysteresis (ΔT) NiTi-based shape memory alloys, with Ti50.0Ni46.7Cu0.8Fe2.3Pd0.2 possessing the smallest ΔT (1.84 K). We synthesize and characterize 36 predicted compositions (9 feedback loops) from a potential space of ∼800,000 compositions. Of these, 14 had smaller ΔT than any of the 22 in the original data set.},
	language = {en},
	number = {1},
	urldate = {2020-09-29},
	journal = {Nature Communications},
	author = {Xue, Dezhen and Balachandran, Prasanna V. and Hogden, John and Theiler, James and Xue, Deqing and Lookman, Turab},
	month = apr,
	year = {2016},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {11241},
	file = {Full Text PDF:/home/awsm/snap/zotero-snap/common/Zotero/storage/ZAVK5XXY/Xue et al. - 2016 - Accelerated search for materials with targeted pro.pdf:application/pdf;Snapshot:/home/awsm/snap/zotero-snap/common/Zotero/storage/4SSEPN5Q/ncomms11241.html:text/html}
}

@article{butler_machine_2018,
	title = {Machine learning for molecular and materials science},
	volume = {559},
	copyright = {2018 Macmillan Publishers Ltd., part of Springer Nature},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-018-0337-2},
	doi = {10.1038/s41586-018-0337-2},
	abstract = {Here we summarize recent progress in machine learning for the chemical sciences. We outline machine-learning techniques that are suitable for addressing research questions in this domain, as well as future directions for the field. We envisage a future in which the design, synthesis, characterization and application of molecules and materials is accelerated by artificial intelligence.},
	language = {en},
	number = {7715},
	urldate = {2020-09-29},
	journal = {Nature},
	author = {Butler, Keith T. and Davies, Daniel W. and Cartwright, Hugh and Isayev, Olexandr and Walsh, Aron},
	month = jul,
	year = {2018},
	note = {Number: 7715
Publisher: Nature Publishing Group},
	pages = {547--555},
	file = {Full Text PDF:/home/awsm/snap/zotero-snap/common/Zotero/storage/26IKZDIL/Butler et al. - 2018 - Machine learning for molecular and materials scien.pdf:application/pdf;Snapshot:/home/awsm/snap/zotero-snap/common/Zotero/storage/AJFJVZS7/s41586-018-0337-2.html:text/html}
}
